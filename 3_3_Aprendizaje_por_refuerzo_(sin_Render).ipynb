{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igomezv/ANN_Nbody/blob/main/3_3_Aprendizaje_por_refuerzo_(sin_Render).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyzZ12KI8xZS"
      },
      "source": [
        "# Aprendizaje por refuerzo\n",
        "\n",
        "Instalar las siguientes librerias para usar esta notebook:\n",
        "\n",
        "!pip install swig\n",
        "\n",
        "!pip install cmake pygame gym[all]\n",
        "\n",
        "!pip install stable_baselines3\n",
        "\n",
        "Referencia: https://www.gocoder.one/blog/rl-tutorial-with-openai-gym\n",
        "\n",
        "![](https://raw.githubusercontent.com/igomezv/DataScienceIntermedio/main/img/refuerzo.png)\n",
        "Fuente de la imagen: medium.com\n",
        "\n",
        "![](https://raw.githubusercontent.com/igomezv/DataScienceIntermedio/main/img/RLdog.jpg)\n",
        "Fuente: kdnuggets.com\n",
        "\n",
        "Objetivo: maximizar la recompenza.\n",
        "\n",
        "**Definiciones importantes:**\n",
        "\n",
        "- Agente: modelo a entrenar para que tome decisiones.\n",
        "- Ambiente: Entorno donde interactúa el agente. El ambiente limita y pone reglas.\n",
        "\n",
        "Entre ambiente y agente hay las siguientes relaciones:\n",
        "\n",
        "- Acción: posibles acciones que puede tomar el agente en determinado momento.\n",
        "- Estado (del ambiente): indicadores del ambiente sobre cómo están los diversos elementos que lo componen en determinado momento.\n",
        "- Recompensas (o penalización): cada acción del agente obtiene un premio o una penalización.\n",
        "\n",
        "Ver: https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/igomezv/DataScienceIntermedio/main/img/RLalg.png)\n",
        "Fuente: medium.com\n",
        "\n",
        "#### Usar el entorno TAXI\n",
        "\n",
        "**Contexto:** Taxi es un entorno disponible en Open-IA gym (https://www.gymlibrary.dev/environments/toy_text/taxi/).\n",
        "\n",
        "- El objetivo de Taxi es recoger pasajeros y dejarlos en el destino en la menor cantidad de movimientos.\n",
        "\n",
        "- Se comparara un agente de taxis que realiza acciones al azar con uno entrenado mediante Aprendizaje Reforzado.\n",
        "\n",
        "- Acciones: moverse hacia arriba, abajo, izquierda, derecha, recoger o dejar pasajeros.\n",
        "\n",
        "Descripción de la documentación:\n",
        "\n",
        "Hay cuatro ubicaciones designadas en el mundo de la cuadrícula indicadas por R (rojo), G (verde), Y (amarillo) y B (azul). Cuando el episodio inicia, el taxi parte de una casilla aleatoria y el pasajero se encuentra en una ubicación aleatoria. El taxi conduce hasta la ubicación del pasajero, lo recoge, conduce hasta el destino del pasajero (otra de las cuatro ubicaciones especificadas) y luego deja al pasajero. Una vez que se deja al pasajero, el episodio termina.\n",
        "\n",
        "Map:\n",
        "\n",
        "+---------+\n",
        "\n",
        "|R: | : :G|\n",
        "\n",
        "| : | : : |\n",
        "\n",
        "| : : : : |\n",
        "\n",
        "| | : | : |\n",
        "\n",
        "|Y| : |B: |\n",
        "\n",
        "+---------+\n",
        "\n",
        "##### Comportamiento\n",
        "\n",
        "\n",
        "Hay 6 acciones deterministas discretas:\n",
        "\n",
        "    0: mover al sur\n",
        "\n",
        "    1: mover al norte\n",
        "\n",
        "    2: moverse hacia el este\n",
        "\n",
        "    3: muévete hacia el oeste\n",
        "\n",
        "    4: pasajero de recogida\n",
        "\n",
        "    5: dejar al pasajero\n",
        "\n",
        "Observaciones\n",
        "\n",
        "Hay 500 estados discretos ya que hay 25 posiciones de taxi, 5 ubicaciones posibles del pasajero (incluido el caso cuando el pasajero está en el taxi) y 4 ubicaciones de destino.\n",
        "\n",
        "Tenga en cuenta que hay 400 estados a los que se puede llegar durante un episodio. Los estados faltantes corresponden a situaciones en las que el pasajero se encuentra en el mismo lugar que su destino, ya que esto suele indicar el final de un episodio. Se pueden observar cuatro estados adicionales justo después de un episodio exitoso, cuando tanto el pasajero como el taxi están en el destino. Esto da un total de 404 estados discretos alcanzables.\n",
        "\n",
        "Cada espacio de estado está representado por la tupla: (fila_taxi, col_taxi, ubicación_pasajero, destino)\n",
        "\n",
        "\n",
        "Ubicaciones de pasajeros:\n",
        "\n",
        "    0: R (rojo)\n",
        "\n",
        "    1: G (verde)\n",
        "\n",
        "    2: Y (amarillo)\n",
        "\n",
        "    3: B (azul)\n",
        "\n",
        "    4: en taxi\n",
        "\n",
        "Destinos:\n",
        "\n",
        "    0: R (rojo)\n",
        "\n",
        "    1: G (verde)\n",
        "\n",
        "    2: Y (amarillo)\n",
        "\n",
        "    3: B (azul)\n",
        "\n",
        "Recompensas\n",
        "\n",
        "     -1 por paso a menos que se active otra recompensa.\n",
        "\n",
        "     +20 entregando pasajero.\n",
        "\n",
        "     -10 ejecutar acciones de “recogida” y “devolución” de forma ilegal.\n",
        "     \n",
        "     - Una ilegalidad consiste en ejecutar las acciones\"recoger\"o \"devolver\" en lugares donde no hay personas.\n",
        "\n",
        "![](https://www.gocoder.one/static/state-rewards-62ab43a53e07062b531b3199a8bab5b3.png)\n",
        "\n",
        "Como llegar al punto R sin perder tantos puntos?\n",
        "\n",
        "## Q-Learning\n",
        "\n",
        "### Tablas-Q\n",
        "\n",
        "Una tabla Q es una tabla de búsqueda que almacena valores que representan las recompensas futuras máximas esperadas que el agente puede esperar por una acción determinada en un estado determinado (conocidos como valores Q).\n",
        "\n",
        "![](https://i.stack.imgur.com/Bn6MY.gif)\n",
        "\n",
        "Estas tablas se suelen inicializar en 0s\n",
        "\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "\n",
        "Hyperparámetros:\n",
        "\n",
        "- Tasa de aprendizaje (α): qué tanto el agente debe aceptar nueva información sobre la información previamente aprendida\n",
        "\n",
        "- Factor de descuento (γ): qué tanto el agente debe considerar las recompensas que podría recibir en el futuro frente a su recompensa inmediata.\n",
        "\n",
        "#### Primero, analicemos el comportamiento aleatorio"
      ],
      "id": "zyzZ12KI8xZS"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cmake pygame gym[all]\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "aQ4ylQkQ86aV",
        "outputId": "6229120d-db88-4fd1-89e4-0ee6cc187eed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aQ4ylQkQ86aV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Using cached swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "Installing collected packages: swig\n",
            "Successfully installed swig-4.1.1\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.27.7)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[all])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mujoco-py<2.2,>=2.1 (from gym[all])\n",
            "  Using cached mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (3.7.1)\n",
            "Collecting lz4>=3.1.0 (from gym[all])\n",
            "  Using cached lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (4.8.0.76)\n",
            "Collecting mujoco==2.2.0 (from gym[all])\n",
            "  Using cached mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.31.6)\n",
            "Collecting pygame\n",
            "  Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[all]) (4.1.1)\n",
            "Collecting pytest==7.0.1 (from gym[all])\n",
            "  Using cached pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "Collecting ale-py~=0.7.5 (from gym[all])\n",
            "  Using cached ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (1.4.0)\n",
            "Collecting glfw (from mujoco==2.2.0->gym[all])\n",
            "  Using cached glfw-2.6.2-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (208 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (3.1.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (1.3.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.0.1->gym[all])\n",
            "  Using cached py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[all]) (6.1.0)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gym[all]) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.16.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gym[all])\n",
            "  Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[all]) (1.16.0)\n",
            "Building wheels for collected packages: box2d-py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dslaz9kW9V_7"
      },
      "id": "dslaz9kW9V_7"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZF7JGuc58xZU"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "ZF7JGuc58xZU"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mXZDT7P-8xZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281b79fa-bb8f-42b6-cd31-14fd1e252e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "id": "mXZDT7P-8xZU"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1BASXRSh8xZV"
      },
      "outputs": [],
      "source": [
        "# create a new instance of taxi, and get the initial state\n",
        "state = env.reset()"
      ],
      "id": "1BASXRSh8xZV"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hs_7GdQ68xZV"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def randomtaxi():\n",
        "    total_reward = 0\n",
        "    # crear entorno de Taxi\n",
        "    env = gym.make('Taxi-v3')\n",
        "\n",
        "    # crear una instancia de Taxi, y obtener un estado inicial\n",
        "    state = env.reset()\n",
        "\n",
        "    num_steps = 9\n",
        "\n",
        "    for s in range(num_steps+1):\n",
        "        print(\"Paso: {}/{}\".format(s+1, num_steps+1))\n",
        "\n",
        "        # muestrear una accion aleatoria de la lista de acciones posibles\n",
        "        action = env.action_space.sample()\n",
        "        # ejecutar esta accion sobre el entorno\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        # Update to our new state\n",
        "        state = new_state\n",
        "        total_reward += reward\n",
        "        print(\"Accion {} Estado {} Recompensa {} Total rec {}\".format(action, state, reward, total_reward))\n",
        "\n",
        "\n",
        "        # imprimir nuevo estado\n",
        "        env.render()\n",
        "\n",
        "        # if done, finish episode\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "\n",
        "    # terminar y cerrar el entorno del taxi\n",
        "    env.close()\n",
        "    return total_reward"
      ],
      "id": "hs_7GdQ68xZV"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9PDkmwqj8xZW",
        "outputId": "2f890a1a-06e5-4c5d-bb30-480f488c9c95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paso: 1/10\n",
            "Accion 4 Estado 96 Recompensa -1 Total rec -1\n",
            "Paso: 2/10\n",
            "Accion 3 Estado 76 Recompensa -1 Total rec -2\n",
            "Paso: 3/10\n",
            "Accion 2 Estado 96 Recompensa -1 Total rec -3\n",
            "Paso: 4/10\n",
            "Accion 3 Estado 76 Recompensa -1 Total rec -4\n",
            "Paso: 5/10\n",
            "Accion 0 Estado 176 Recompensa -1 Total rec -5\n",
            "Paso: 6/10\n",
            "Accion 1 Estado 76 Recompensa -1 Total rec -6\n",
            "Paso: 7/10\n",
            "Accion 0 Estado 176 Recompensa -1 Total rec -7\n",
            "Paso: 8/10\n",
            "Accion 2 Estado 196 Recompensa -1 Total rec -8\n",
            "Paso: 9/10\n",
            "Accion 0 Estado 296 Recompensa -1 Total rec -9\n",
            "Paso: 10/10\n",
            "Accion 5 Estado 296 Recompensa -10 Total rec -19\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-19"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "randomtaxi()"
      ],
      "id": "9PDkmwqj8xZW"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TE0Y9iGi8xZW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "def rltaxi():\n",
        "\n",
        "    # create Taxi environment\n",
        "    env = gym.make('Taxi-v3')\n",
        "\n",
        "    # initialize q-table\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "    qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "    # hyperparameters\n",
        "    learning_rate = 0.9\n",
        "    discount_rate = 0.8\n",
        "    epsilon = 1.0\n",
        "    decay_rate= 0.005\n",
        "\n",
        "    # training variables\n",
        "    num_episodes = 9\n",
        "    max_steps = 99 # per episode\n",
        "\n",
        "    # training\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # reset the environment\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        for s in range(max_steps):\n",
        "\n",
        "            # exploration-exploitation tradeoff\n",
        "            if random.uniform(0,1) < epsilon:\n",
        "                # explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # exploit\n",
        "                action = np.argmax(qtable[state,:])\n",
        "\n",
        "            # take action and observe reward\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Q-learning algorithm\n",
        "            qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "            # Update to our new state\n",
        "            state = new_state\n",
        "\n",
        "            # if done, finish episode\n",
        "            if done == True:\n",
        "                break\n",
        "\n",
        "        # Decrease epsilon\n",
        "        epsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "    print(f\"Training completed over {num_episodes} episodes\")\n",
        "    input(\"Press Enter to watch trained agent...\")\n",
        "\n",
        "    # watch trained agent\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for s in range(max_steps+1):\n",
        "\n",
        "        print(f\"TRAINED AGENT\")\n",
        "        print(\"Paso {}\".format(s+1))\n",
        "\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        env.render()\n",
        "\n",
        "        total_reward += reward\n",
        "        print(\"Accion {} Estado {} Recompensa {} Total rec {}\".format(action, state, reward, total_reward))\n",
        "        state = new_state\n",
        "#comentar esto y volver a correr\n",
        "        if done == True:\n",
        "          break\n",
        "\n",
        "    env.close()\n"
      ],
      "id": "TE0Y9iGi8xZW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfuys0qO8xZX",
        "outputId": "1a864cb7-cea5-4734-83dd-09c488c07aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed over 9 episodes\n",
            "Press Enter to watch trained agent...\n",
            "TRAINED AGENT\n",
            "Paso 1\n",
            "Accion 0 Estado 381 Recompensa -1 Total rec -2\n",
            "TRAINED AGENT\n",
            "Paso 2\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -4\n",
            "TRAINED AGENT\n",
            "Paso 3\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -6\n",
            "TRAINED AGENT\n",
            "Paso 4\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -8\n",
            "TRAINED AGENT\n",
            "Paso 5\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -10\n",
            "TRAINED AGENT\n",
            "Paso 6\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -12\n",
            "TRAINED AGENT\n",
            "Paso 7\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -14\n",
            "TRAINED AGENT\n",
            "Paso 8\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -16\n",
            "TRAINED AGENT\n",
            "Paso 9\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -18\n",
            "TRAINED AGENT\n",
            "Paso 10\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -20\n",
            "TRAINED AGENT\n",
            "Paso 11\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -22\n",
            "TRAINED AGENT\n",
            "Paso 12\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -24\n",
            "TRAINED AGENT\n",
            "Paso 13\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -26\n",
            "TRAINED AGENT\n",
            "Paso 14\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -28\n",
            "TRAINED AGENT\n",
            "Paso 15\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -30\n",
            "TRAINED AGENT\n",
            "Paso 16\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -32\n",
            "TRAINED AGENT\n",
            "Paso 17\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -34\n",
            "TRAINED AGENT\n",
            "Paso 18\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -36\n",
            "TRAINED AGENT\n",
            "Paso 19\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -38\n",
            "TRAINED AGENT\n",
            "Paso 20\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -40\n",
            "TRAINED AGENT\n",
            "Paso 21\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -42\n",
            "TRAINED AGENT\n",
            "Paso 22\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -44\n",
            "TRAINED AGENT\n",
            "Paso 23\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -46\n",
            "TRAINED AGENT\n",
            "Paso 24\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -48\n",
            "TRAINED AGENT\n",
            "Paso 25\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -50\n",
            "TRAINED AGENT\n",
            "Paso 26\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -52\n",
            "TRAINED AGENT\n",
            "Paso 27\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -54\n",
            "TRAINED AGENT\n",
            "Paso 28\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -56\n",
            "TRAINED AGENT\n",
            "Paso 29\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -58\n",
            "TRAINED AGENT\n",
            "Paso 30\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -60\n",
            "TRAINED AGENT\n",
            "Paso 31\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -62\n",
            "TRAINED AGENT\n",
            "Paso 32\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -64\n",
            "TRAINED AGENT\n",
            "Paso 33\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -66\n",
            "TRAINED AGENT\n",
            "Paso 34\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -68\n",
            "TRAINED AGENT\n",
            "Paso 35\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -70\n",
            "TRAINED AGENT\n",
            "Paso 36\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -72\n",
            "TRAINED AGENT\n",
            "Paso 37\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -74\n",
            "TRAINED AGENT\n",
            "Paso 38\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -76\n",
            "TRAINED AGENT\n",
            "Paso 39\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -78\n",
            "TRAINED AGENT\n",
            "Paso 40\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -80\n",
            "TRAINED AGENT\n",
            "Paso 41\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -82\n",
            "TRAINED AGENT\n",
            "Paso 42\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -84\n",
            "TRAINED AGENT\n",
            "Paso 43\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -86\n",
            "TRAINED AGENT\n",
            "Paso 44\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -88\n",
            "TRAINED AGENT\n",
            "Paso 45\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -90\n",
            "TRAINED AGENT\n",
            "Paso 46\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -92\n",
            "TRAINED AGENT\n",
            "Paso 47\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -94\n",
            "TRAINED AGENT\n",
            "Paso 48\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -96\n",
            "TRAINED AGENT\n",
            "Paso 49\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -98\n",
            "TRAINED AGENT\n",
            "Paso 50\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -100\n",
            "TRAINED AGENT\n",
            "Paso 51\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -102\n",
            "TRAINED AGENT\n",
            "Paso 52\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -104\n",
            "TRAINED AGENT\n",
            "Paso 53\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -106\n",
            "TRAINED AGENT\n",
            "Paso 54\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -108\n",
            "TRAINED AGENT\n",
            "Paso 55\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -110\n",
            "TRAINED AGENT\n",
            "Paso 56\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -112\n",
            "TRAINED AGENT\n",
            "Paso 57\n",
            "Accion 0 Estado 481 Recompensa -1 Total rec -114\n",
            "TRAINED AGENT\n",
            "Paso 58\n"
          ]
        }
      ],
      "source": [
        "rltaxi()"
      ],
      "id": "Kfuys0qO8xZX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7858psy8xZY"
      },
      "source": [
        "###### Ejercicio comparar recompensas de ambos metodos con 3 diferentes numeros de iteraciones."
      ],
      "id": "_7858psy8xZY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0fKMtzD8xZZ"
      },
      "outputs": [],
      "source": [],
      "id": "P0fKMtzD8xZZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}